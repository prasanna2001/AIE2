{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "metadata": {},
        "outputId": "894efab9-f2d7-4f2b-fe9a-b9d57152eace"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FMJqq8SYt34V"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "metadata": {},
        "outputId": "d56a1a4e-4175-4522-9100-2261f412efbb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "metadata": {},
        "outputId": "23327edb-4f59-4d6c-e145-54721b0836ad"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "uyFgZVUSEexW",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://xthi0qh157vn0hd4.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/XyZhOv8.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "metadata": {},
        "outputId": "136a97cf-8faa-45de-fe1e-2c1449971d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Hello! How are you? I hope you are doing well. I am doing well. I am happy to be here. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy to be here with you. I am happy'}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "metadata": {},
        "outputId": "73f7b340-5156-4f6b-fd1f-11e588b49337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/prasanna2001/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "mMJrWnKISFqb",
        "metadata": {},
        "outputId": "d661643a-b611-4766-a84e-55afeb473d91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" I am doing great. I hope you are doing well too.\\nIt has been a long time since I posted a blog post. My son has been getting more and more busier with his studies. So I was not able to post regularly. I am also trying to get back to my normal routine as well. I hope you all are having a good time.\\nHere are some of my favorites for this week.\\nThat's all for today. Have a great day.\""
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "wrZJHVGkGLZr",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://d0izn1430j0kx0wz.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4asz9Ofn0MtP",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "metadata": {},
        "outputId": "56958042-8212-406f-b378-ed018f5ffad2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.026445432,\n",
              " 0.035848815,\n",
              " 0.009444274,\n",
              " 0.011648565,\n",
              " 0.006504409,\n",
              " 0.008241863,\n",
              " -0.036926176,\n",
              " -0.03630842,\n",
              " -0.024835369,\n",
              " -0.0053979824]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ‚ùì Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "###Answer\n",
        "109M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "3b637172-2b49-4f51-fb02-77f7609e677b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1305"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ‚ùì Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "###Answers\n",
        "Limiting batch sizes when sending data to Hugging Face endpoints ensures compliance with API rate limits, optimizes memory usage, improves latency, and prevents server-side errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "bfdc1130-e26f-49c1-cbc1-79a9389a8cbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prasanna2001/anaconda3/envs/llmops-course/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='We have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model'),\n",
              " Document(page_content='of QDyLoRA through several instruct-fine-tuning\\nTable 3: Comparing the performance of DyLoRA, QLoRA and QDyLoRA across different evaluation ranks. all'),\n",
              " Document(page_content='QLoRA delivers convincing accuracy improvements across\\nthe LLaMA and LLaMA2 families, even with 2-4 bit-widths,\\naccompanied by a minimal 0.45% increase in time con-\\nsumption. Remarkably versatile, IR-QLoRA seamlessly'),\n",
              " Document(page_content='performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]\\n‚àóEqual contribution.'),\n",
              " Document(page_content='Moreover, QLoRA is trained\\non a pre-defined rank and, therefore, cannot\\nbe reconfigured for its lower ranks without\\nrequiring further fine-tuning steps. This pa-\\nper proposes QDyLoRA -Quantized Dynamic\\nLow-Rank Adaptation-, as an efficient quantiza-')]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ‚ùì Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "### Answers\n",
        "Ordering of prompt is necessary to achieve logical and accurate responses, maintain a consistent and logical ordering of instructions, context, and questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "c60a6f33-31a5-4b44-dbeb-647a7a94e1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLORA stands for Quantitative Linguistic Optimized Representation Allocation.\\n\\nAnswer:\\nQLORA is a method for optimizing the parameters of a neural network model for text generation.\\n\\nAnswer:\\nThe main purpose of QLORA is to reduce the memory footprint required for finetuning models.\\n\\nAnswer:\\nYes, QLORA can perform as well as full-model finetuning for text generation.\\n\\nAnswer:\\nQLORA is not as efficient for finetuning as full-model finetuning.\\n\\nAnswer:\\nQLORA is a method for optimizing the parameters of a neural network model for text generation.\\n\\nAnswer:\\nQDyLoRA can be used for text generation, but its performance is not as good as full-model finetuning.\\n\\nAnswer:\\nYes, QLORA is able to perform as well as full-model finetuning for text generation.\\n\\nAnswer:\\nNo, QLORA is not as efficient for finetuning as full-model finetuning.\\n\\nAnswer:\\nQLORA is a method for optimizing the parameters of a neural network model for text generation.\\n\\nAnswer:\\nQDyLoRA can be used for text generation, but its performance is not as good as full-model finetuning.\\n\\nAnswer:\\nYes, QLORA is able to perform as well as full-model finetuning for text generation.\\n\\nAnswer:\\nNo, QLORA is not as efficient for finetuning as full-model finetuning.\\n\\nAnswer:\\nQLORA is a method for optimizing the parameters of a neural network model for text generation.\\n\\nAnswer:\\nQDyLoRA can be used for text generation, but its performance is not as good as full-model finetuning.\\n\\nAnswer:\\nYes, QLORA is able to perform as well as full-model finetuning for text generation.\\n\\nAnswer:\\nNo, QLORA is not as efficient for finetuning as full-model finetuning.\\n\\nAnswer:\\nQLORA is a method for optimizing the parameters of a neural network model for text generation.\\n\\nAnswer:\\nQDyLoRA can be used for text generation, but its performance is not as good'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLORA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "2cec0c3e-10c2-4c34-b7c6-21fadd39a2e7"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "1108b929-4e98-4f85-8c68-7e5e5368e54e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n```\\n\\n# How to use:\\npython run_classification_qa.py\\n```\\n'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use? 285 tokens\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete? 2.55secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset v2\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "6b724916-154a-4d71-c161-8c4e186f46f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v1' at:\n",
            "https://smith.langchain.com/o/6e717344-7e25-5f4e-995f-5a3845883c52/datasets/8b0c8c7b-81b1-4d28-8795-fd945b414d1e/compare?selectedSessions=5dc4cd93-8249-469a-a0a6-66c7bcd569cb\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset v2 at:\n",
            "https://smith.langchain.com/o/6e717344-7e25-5f4e-995f-5a3845883c52/datasets/8b0c8c7b-81b1-4d28-8795-fd945b414d1e\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.harmfulness</th>\n",
              "      <th>feedback.AI</th>\n",
              "      <th>feedback.scored_dopeness</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dc7150a3-db4e-4062-9a91-044aa2800ddf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.425065</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.547723</td>\n",
              "      <td>0.367990</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.074588</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.423523</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.891623</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.296717</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.336424</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.093846</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.harmfulness  feedback.AI  \\\n",
              "count               6.000000                   6.0     6.000000   \n",
              "unique                   NaN                   NaN          NaN   \n",
              "top                      NaN                   NaN          NaN   \n",
              "freq                     NaN                   NaN          NaN   \n",
              "mean                0.166667                   0.0     0.500000   \n",
              "std                 0.408248                   0.0     0.547723   \n",
              "min                 0.000000                   0.0     0.000000   \n",
              "25%                 0.000000                   0.0     0.000000   \n",
              "50%                 0.000000                   0.0     0.500000   \n",
              "75%                 0.000000                   0.0     1.000000   \n",
              "max                 1.000000                   0.0     1.000000   \n",
              "\n",
              "        feedback.scored_dopeness error  execution_time  \\\n",
              "count                   6.000000     0        6.000000   \n",
              "unique                       NaN     0             NaN   \n",
              "top                          NaN   NaN             NaN   \n",
              "freq                         NaN   NaN             NaN   \n",
              "mean                    0.458333   NaN       13.425065   \n",
              "std                     0.367990   NaN       14.074588   \n",
              "min                     0.000000   NaN        2.423523   \n",
              "25%                     0.137500   NaN        4.891623   \n",
              "50%                     0.550000   NaN        8.296717   \n",
              "75%                     0.737500   NaN       15.336424   \n",
              "max                     0.850000   NaN       40.093846   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     dc7150a3-db4e-4062-9a91-044aa2800ddf  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v1',\n",
              " 'results': {'6860557d-505a-47e7-b839-458b6e8d9ef1': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion is \"helpfulness: Is the submission helpful, insightful, and appropriate?\"\\n\\nStep 1: Assessing Helpfulness - The submission does provide an answer to the question, indicating that the Adam optimizer is used in QLoRA. This could be helpful if it is correct.\\n\\nStep 2: Assessing Insightfulness - The submission does not provide any additional information or insight beyond the direct answer to the question. It does not explain why the Adam optimizer is used or what it does.\\n\\nStep 3: Assessing Appropriateness - The submission is appropriate as it directly answers the question asked.\\n\\nHowever, the answer is incorrect. QLoRA doesn\\'t use Adam optimizer. Therefore, the submission is not helpful or insightful.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('be748f4a-3332-47a2-9c57-391e36d44b6e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple, straightforward answer to a technical question about a specific optimizer used in QLoRA. \\n\\nThere is no language or content that could be considered harmful, offensive, or inappropriate. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ca0ce386-e412-45fa-9d44-299c0658576e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking if the response feels like it was generated by an AI. The response provided is \"We use Adam optimizer.\" This response does not exhibit any signs of being AI-generated. It is a straightforward and concise answer to the question asked. It uses human-like language and does not contain any errors or unusual phrasing that might suggest it was generated by an AI. Therefore, the response does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3755f889-ac0d-4f37-b352-135f61141290'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment=\"The response is accurate and directly answers the question asked. It uses clear and concise language, making it easy to understand. However, it lacks any additional information or context that could make it more interesting or engaging. For example, it could have included a brief explanation of what the Adam optimizer is or why it's used in QLoRA.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f6c586d5-9a09-417d-a799-2c29847c4707'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.423523,\n",
              "   'run_id': 'dc7150a3-db4e-4062-9a91-044aa2800ddf',\n",
              "   'output': '\\nAnswer: We use Adam optimizer.\\n'},\n",
              "  '34a36e2f-db18-4015-8e74-1de0c85108f1': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment=\"The criterion is about the helpfulness, insightfulness, and appropriateness of the submission. \\n\\nLooking at the submission, it provides an answer to the input question. The answer seems to be in a format that suggests a data type with specific attributes. However, the question asks about a data type created in the QLoRA paper, and the answer provided is about a data type named 'Bfloat16'. \\n\\nWithout context, it's hard to determine if 'Bfloat16' is indeed the data type created in the QLoRA paper. If it is, then the answer would be helpful and appropriate. However, if 'Bfloat16' is not the data type created in the QLoRA paper, then the answer would not be helpful or appropriate. \\n\\nThe answer could be more insightful if it included some information about why this data type was created or what it's used for in the context of the QLoRA paper. \\n\\nGiven the lack of context and the potential for the answer to be incorrect, it's difficult to definitively say that the submission meets the criteria. \\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bbe6cdcc-7114-4192-9845-4f1034c51ea9'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a technical response about a data type created in a specific paper. There is no language or content that could be considered harmful, offensive, or inappropriate. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5ca5ae5a-00a7-462c-870e-3f89dfc634d7'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion asks if the response feels like it was generated by an AI. The response provided is in a structured format that resembles a data object or a class instance in programming. This is not a typical way a human would respond to the question, but rather how a machine or AI might structure the information. Therefore, the response does feel AI generated.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fb347323-58d1-4830-8f17-b61272751c7e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is accurate and informative. It provides the exact data type that was created in the QLoRA paper, which is Bfloat16. It also provides additional information about the data type, such as it being a float32 type and having a precision of 16. However, the response could be more engaging and less technical to appeal to a wider audience. It could also provide some context or explanation about what the QLoRA paper is and why this data type is significant.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e9e0376d-bf14-4dff-a161-c5b9717e78ff'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.917337,\n",
              "   'run_id': '5013cebf-d568-4490-8e1a-9fd1ff7f0911',\n",
              "   'output': \"\\nAnswer: [Data Type(name='Bfloat16', data_type='float32', precision=16)]\\n\\n\"},\n",
              "  '330506f5-826e-4c99-8efd-fcd1fde70732': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of what a Retrieval Augmented Generation system is. It explains the capabilities of the system, its uses, and its applications. The information provided is relevant to the question and is likely to be helpful to someone seeking to understand what a Retrieval Augmented Generation system is. \\n\\nThe submission is insightful as it not only defines the system but also explains its uses and applications. This gives a deeper understanding of the system and its importance. \\n\\nThe submission is appropriate as it directly answers the question and stays on topic. It does not include any irrelevant or inappropriate information. \\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0b470bb1-9141-470e-8c79-9e10f70d6076'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of what a Retrieval Augmented Generation system is. It provides information about its capabilities, uses, and applications. \\n\\nThere is no content in the submission that can be considered harmful, offensive, or inappropriate. The submission is purely informational and technical in nature.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e5e7ede8-3268-4801-8b5c-d267698f08f0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. \\n\\nThe response provided is coherent, detailed, and accurate. It provides a comprehensive explanation of what a Retrieval Augmented Generation system is, including its uses and applications. \\n\\nHowever, the language used in the response is quite formal and technical, which could potentially suggest that it was generated by an AI. \\n\\nOn the other hand, the response does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or a lack of coherence. \\n\\nTherefore, it is not definitively clear whether the response was generated by an AI or not. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('da514eea-4d71-4f53-8bfe-5ecd4002da0e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.75, value=None, comment='The response is very informative and detailed, providing a clear and comprehensive explanation of what a Retrieval Augmented Generation system is. It also gives examples of its applications, which helps to further understand its uses and importance. The language used is professional and appropriate for the context. However, it lacks a bit of creativity and excitement that could make it more engaging and interesting to read. It\\'s more of a textbook definition than a \"dope\" response.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c3b7c0be-352f-4c67-b649-7f711ad7863d'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 11.676098,\n",
              "   'run_id': 'f772f18d-5dcb-4d75-ae09-dc82d00cd66a',\n",
              "   'output': '\\nAnswer:\\nA Retrieval Augmented Generation system is a type of language model that combines the capabilities of both retrieval and generation. It can be used to produce high-quality, coherent, and relevant text from a large corpus of documents. It is often used for tasks such as natural language processing, information retrieval, and text summarization. It is also used for applications such as question answering, machine translation, and video understanding. It can be used to generate target output for given prompts, enabling the cost-effective production of large-scale synthetic datasets for training.\\n\\n'},\n",
              "  '1f889b87-ec15-4071-956c-acbb4d0fab0e': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the user asked for the authors of the QLoRA paper. The response provided links to images of the authors of three different papers: QLoRA, DyLoRA, and QDyLoRA. \\n\\nThe submission does not directly answer the question. Instead, it provides links to images which may or may not contain the correct information. This requires the user to click on each link and check the information themselves, which is not very helpful. \\n\\nFurthermore, the submission includes information about the authors of two other papers (DyLoRA and QDyLoRA) which were not asked for. This is not appropriate as it does not directly answer the question asked.\\n\\nTherefore, the submission does not meet the criterion of being helpful, insightful, and appropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('34a4dd4e-2f56-4022-89f2-0ffdeba8a59f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it appears to be a response to the question about the authors of the QLoRA paper. The response includes links to images that supposedly contain the names of the authors of the QLoRA, DyLoRA, and QDyLoRA papers. \\n\\nThere is no content in the submission that can be considered harmful, offensive, or inappropriate. The submission seems to be purely informational and related to the question asked.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0a952805-de5c-43f5-8902-416dcd42f0f2'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels AI generated.\\n\\nLooking at the submission, the response does not seem to be AI generated. The response includes images from a GitHub repository, which is not a typical output from an AI model. AI models usually generate text-based responses and do not have the capability to generate or include images from external sources in their responses. \\n\\nTherefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b4f22d75-e191-4dc5-9abd-660439221820'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.4, value=None, comment=\"The response is not very dope because it doesn't directly answer the question asked. The input asked for the authors of the QLoRA paper, but the output provides links to images for the authors of three different papers, including the QLoRA paper. This requires the user to click on the links to get the information, which is not very user-friendly. Also, the output includes information about the authors of the DyLoRA and QDyLoRA papers, which were not asked for in the input. This is irrelevant information in the context of the question. However, the response does technically provide the answer to the question, so it's not completely unhelpful.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('16a6a49d-e3c4-4611-beb8-9f2734e7902f'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 16.556533,\n",
              "   'run_id': 'dab1e847-0a72-4a1d-9e7f-9fd2f796dad1',\n",
              "   'output': '\\nAnswer:\\nThe authors of the QLoRA paper are:\\n\\n![](https://github.com/Akash-Kumar-Saraf/GPT-3-Prompting/blob/main/questions/Paper_authors.png)\\n\\nThe authors of the DyLoRA paper are:\\n\\n![](https://github.com/Akash-Kumar-Saraf/GPT-3-Prompting/blob/main/questions/DyLoRA_authors.png)\\n\\nThe authors of the QDyLoRA paper are:\\n\\n![](https://github.com/Akash-Kumar-Saraf/GPT-3-Prompting/blob/main/questions/QDyLoRA_authors.png)\\n\\n\\n\\n\\n'},\n",
              "  '9ad6203e-469e-4085-9ba6-e40b9ff1cd37': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criteria for this task is \"helpfulness\". The submission should provide a helpful, insightful, and appropriate response to the input question.\\n\\nLooking at the input, the question asked is \"What is the most popular deep learning framework?\" This is a clear and direct question expecting a specific answer.\\n\\nHowever, the submission does not provide any answer to the question. Instead, it contains empty fields for \"Document\", \"Context\", and \"Question\". This does not provide any helpful or insightful information in response to the input question. It is also not appropriate as it does not address the question at all.\\n\\nTherefore, the submission does not meet the criteria. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8d83ff1a-df3f-44ad-a4da-f94d11cc9ab1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this task is to assess whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it appears to be incomplete or incorrectly formatted, but there is no content that could be considered harmful, offensive, or inappropriate. The submission does not contain any offensive language, harmful advice, or inappropriate content. \\n\\nTherefore, based on the given criterion, the submission is not harmful, offensive, or inappropriate. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5c3392bc-6353-49dd-a2a8-f24d7b66e804'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criteria asks if the response feels AI generated. The submission provided does not contain any answer to the input question. Instead, it contains empty fields for \"Document\", \"Context\", and \"Question\". This could be a result of an AI error or malfunction, as it does not provide a coherent or relevant response to the input. Therefore, it does feel AI generated, but not in a sophisticated or human-like manner.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4cc42fd8-fb75-486b-817c-fa9629ac4275'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.0, value=None, comment=\"The response to the input is not dope at all because it doesn't provide any answer to the question asked. The output is just a series of empty fields, which is not helpful or informative in any way. Therefore, the score would be 0 out of 100.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c1db9048-15c2-445d-b059-1950f730a90e'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.883052,\n",
              "   'run_id': 'c8810757-1635-4d43-84c4-11006faed137',\n",
              "   'output': '\\n\\n## Contexts\\n```\\n\\n### Document:\\n\\n\\n```\\n\\n### Context:\\n\\n\\n```\\n\\n### Question:\\n\\n\\n```\\n'},\n",
              "  'bb53088c-5483-4fc6-af29-f8cc1714141c': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, it starts off well by listing some improvements made by the LoRA system. However, the same points are repeated multiple times without any additional information or context. This repetition does not add any value or insight to the answer. \\n\\nThe repetition also makes the submission less helpful because it does not provide a comprehensive list of improvements made by the LoRA system. Instead, it just repeats the same few points over and over again. \\n\\nThe submission is also not appropriate because it does not fully answer the question. The question asks for significant improvements made by the LoRA system, but the submission only provides a few points and repeats them multiple times. \\n\\nBased on these observations, the submission does not meet the criterion of being helpful, insightful, and appropriate. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('af9b01ee-7a68-4da9-be6e-e280e47370f9'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the content is not offensive or inappropriate. It is a response to a question about the LoRA system and its improvements. \\n\\nHowever, the submission is repetitive, listing the same points multiple times. This could be seen as a form of spam, but it does not seem to be harmful in the sense of causing damage or injury. \\n\\nTherefore, based on the given criterion, the submission is not harmful, offensive, or inappropriate. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('894324c8-4059-4c0f-bcb7-fe70b80df4c3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion asks if the response feels AI generated. The submission repeats the same points multiple times in a manner that does not seem natural or human-like. This repetition and lack of variation in the response suggests that it may have been generated by an AI.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('38d1cdd1-b7aa-4f99-a74b-e5580c97c151'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.05, value=None, comment='The response is not dope at all. It is repetitive and does not provide any new or unique information after the first few points. The same three points are repeated over and over again, which makes the response monotonous and uninteresting. It also does not provide any specific details about how the LoRA system makes these improvements, which would have made the response more informative and engaging. The response also cuts off at the end, which makes it seem incomplete. Therefore, the score is very low.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('65df6bfa-6b4b-4705-8105-741169d8eef6'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 40.093846,\n",
              "   'run_id': '579b07fa-d930-4b4d-a117-f05402dbde71',\n",
              "   'output': '\\nAnswer:\\nThe LoRA system makes significant improvements in the following areas:\\n\\n* Reduces the number of parameters required by a linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount of memory required by a linear projection\\n\\n* Enables the use of more adapters in the linear projection\\n\\n* Improves performance on downstream tasks\\n\\n* Reduces the amount'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v1\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
